{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from kerastuner import tuners\n",
    "from kerastuner import HyperParameters\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import captchanet\n",
    "\n",
    "data_dir = Path('/home/hadim/.data/Neural_Network/captchanet')\n",
    "dataset_dir = data_dir / 'dataset_v6'\n",
    "\n",
    "train_data_dir = dataset_dir / 'training'\n",
    "val_data_dir = dataset_dir / 'validation'\n",
    "\n",
    "tokenizer_path = dataset_dir / \"tokenizer.json\"\n",
    "\n",
    "log_dir = data_dir / 'log'\n",
    "log_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "model_dir = data_dir / 'model'\n",
    "model_dir.mkdir(exist_ok=True, parents=True)\n",
    "  \n",
    "# Get tokenizer\n",
    "with open(tokenizer_path) as f:\n",
    "  #tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n",
    "  from keras_preprocessing import text\n",
    "  tokenizer = text.tokenizer_from_json(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0714 19:35:29.309437 140267627185984 deprecation.py:323] From /home/hadim/conda/envs/captchanet/lib/python3.7/site-packages/tensorflow/python/ops/image_ops_impl.py:1511: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "# Build the dataset\n",
    "def make_dataset(data_dir, tokenizer, batch_size, image_size=None, shuffle=False, n=None):\n",
    "\n",
    "  fnames = [str(p) for p in data_dir.glob(\"*.tfrecord\")]\n",
    "  dataset = tf.data.TFRecordDataset(fnames)\n",
    "  if n:\n",
    "    dataset = dataset.take(n)\n",
    "  if shuffle:\n",
    "      dataset = dataset.shuffle(buffer_size=2048)\n",
    "      \n",
    "  # We could infer it from the dataset but here it's hard-coded.\n",
    "  max_len_word = 10\n",
    "      \n",
    "  decode_fn = captchanet.decode_data(tokenizer, max_len_word, image_size=image_size)\n",
    "  dataset = dataset.map(map_func=decode_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  \n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return dataset\n",
    "\n",
    "batch_size = 24\n",
    "image_size = (299, 299)\n",
    "train_dataset = make_dataset(train_data_dir, tokenizer, batch_size=batch_size, image_size=image_size, shuffle=True, n=None)\n",
    "val_dataset = make_dataset(val_data_dir, tokenizer, batch_size=batch_size, image_size=image_size, shuffle=False, n=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(image_shape, image_type, vocabulary_size, max_len_word, params=None, do_fine_tuning=False, do_build=True):\n",
    "\n",
    "  def _builder(hp):\n",
    "\n",
    "    dropout_rate = hp.Choice('dropout_rate', values=[0.1, 0.2, 0.3, 0.4, 0.5], default=0.2)\n",
    "    use_regularizer = hp.Choice('use_regularizer', values=[True, False], default=True)\n",
    "    regularizer_value = hp.Choice('regularizer_value', values=[1e-1, 1e-2, 1e-3, 1e-4, 1e-5], default=1e-4)\n",
    "    optimizer_name = hp.Choice('optimizer_name', values=['sgd', 'adam', 'rmsprop'], default='sgd')\n",
    "    starting_lr = hp.Choice('starting_lr', values=[1e-1, 1e-2, 1e-3], default=1e-3)\n",
    "    momentum = hp.Choice('momentum', values=[0.9, 0.95, 0.99], default=0.9)\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    #module_selection = (\"mobilenet_v2\", 224, 1280)\n",
    "    module_selection = (\"inception_v3\", 299, 2048)\n",
    "\n",
    "    handle_base, pixels, feature_size = module_selection\n",
    "    module_handle = f\"https://tfhub.dev/google/tf2-preview/{handle_base}/feature_vector/4\"\n",
    "    model_image_shape = (pixels, pixels, 3)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(name='image', shape=model_image_shape, dtype=image_type)\n",
    "    x = inputs\n",
    "\n",
    "    x = hub.KerasLayer(module_handle, output_shape=[feature_size], trainable=do_fine_tuning)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    padding_vocabulary_size = vocabulary_size + 1\n",
    "    outputs = []\n",
    "    for i in range(max_len_word):\n",
    "      if use_regularizer:\n",
    "        regularizer = tf.keras.regularizers.l2(regularizer_value)\n",
    "      else:\n",
    "        regularizer = None\n",
    "      out = tf.keras.layers.Dense(padding_vocabulary_size, kernel_regularizer=regularizer, activation='softmax', name=f'character_{i}')(x)\n",
    "      outputs.append(out)\n",
    "    outputs = tf.keras.layers.Concatenate()(outputs)\n",
    "    outputs = tf.keras.layers.Reshape((max_len_word, padding_vocabulary_size))(outputs)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # Optimizer Parameters\n",
    "    sgd_params = {}\n",
    "    sgd_params['learning_rate'] = starting_lr\n",
    "    sgd_params['momentum'] = momentum\n",
    "    sgd_params['nesterov'] = True\n",
    "\n",
    "    adam_params = {}\n",
    "    adam_params['learning_rate'] = starting_lr\n",
    "    adam_params['amsgrad'] = True\n",
    "\n",
    "    rmsprop_params = {}\n",
    "    rmsprop_params['learning_rate'] = starting_lr\n",
    "\n",
    "    # Build optimizer.\n",
    "    if optimizer_name == 'sgd':\n",
    "      optimizer = tf.keras.optimizers.SGD(**sgd_params)\n",
    "    elif optimizer_name == 'adam':\n",
    "      optimizer = tf.keras.optimizers.Adam(**adam_params)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "      optimizer = tf.keras.optimizers.RMSprop(**rmsprop_params)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "  hp = HyperParameters()\n",
    "\n",
    "  if params:\n",
    "    for key, value in params.items():\n",
    "      hp.Fixed(key, value)\n",
    "      \n",
    "  if not do_build:\n",
    "    return _builder\n",
    "\n",
    "  return _builder(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = []\n",
    "\n",
    "log_path = log_dir / datetime.datetime.now().strftime(\"%Y.%m.%d-%H.%M.%S\")\n",
    "log_path.mkdir(exist_ok=True)\n",
    "\n",
    "tb = captchanet.LRTensorBoard(log_dir=str(log_path), write_images=False, write_graph=True)\n",
    "#callbacks.append(tb)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, min_delta=5e-4, min_lr=1e-6)\n",
    "callbacks.append(reduce_lr)\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=1e-3, patience=10)\n",
    "#callbacks.append(early_stop)\n",
    "\n",
    "chkpt_dir = log_path / 'checkpoints'\n",
    "chkpt_dir.mkdir(exist_ok=True)\n",
    "chkpt_path = chkpt_dir / 'weights_{epoch:02d}_{val_loss:.3f}.hdf5'\n",
    "chkpt = tf.keras.callbacks.ModelCheckpoint(str(chkpt_path), monitor='val_loss', save_best_only=True, save_weights_only=False)\n",
    "callbacks.append(chkpt)\n",
    "\n",
    "tqdm_progress = captchanet.TQDMCallback()\n",
    "callbacks.append(tqdm_progress)\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(str(log_path / 'log.csv'))\n",
    "callbacks.append(csv_logger)\n",
    "\n",
    "# Get image shape\n",
    "image, label = [d for d in train_dataset.take(1)][0]\n",
    "image_shape = image.shape[1:]\n",
    "image_type = image.dtype\n",
    "\n",
    "do_fine_tuning = True\n",
    "\n",
    "params = {}\n",
    "params['dropout_rate'] = 0.4\n",
    "params['use_regularizer'] = True\n",
    "params['regularizer_value'] = 1e-5\n",
    "params['optimizer_name'] = 'adam'\n",
    "params['starting_lr'] = 1e-2\n",
    "params['momentum'] = 0.95\n",
    "\n",
    "# Build the model.\n",
    "vocabulary_size = len(tokenizer.index_word)\n",
    "model = build_model(image_shape, image_type, vocabulary_size, max_len_word=10, params=params, do_fine_tuning=do_fine_tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d8e8494aca464a8f98cd2093553eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=500, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4642ba9387ee48139e52b9a3d69ba3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Epoch: 0', max=1, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=n_epochs, callbacks=callbacks, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_path = model_dir / 'v7'\n",
    "model_path.mkdir(exist_ok=True)\n",
    "model.save(str(model_path / 'model'))\n",
    "\n",
    "# Save tokenizer\n",
    "with open(model_path / tokenizer_path.name, 'w') as f:\n",
    "  f.write(tokenizer.to_json())\n",
    "\n",
    "# Save history\n",
    "history = pd.DataFrame(model.history.history)\n",
    "history_path = model_path / 'history.csv'\n",
    "history.to_csv(str(history_path), index=False)\n",
    "\n",
    "# Pack and zip the model directory\n",
    "import shutil\n",
    "archive_path = model_dir / model_path.stem\n",
    "shutil.rmtree(model_path / '.ipynb_checkpoints', ignore_errors=True)\n",
    "shutil.make_archive(archive_path, 'zip', root_dir=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check history\n",
    "history = pd.read_csv(history_path)\n",
    "\n",
    "n = 3\n",
    "size = 3.5\n",
    "ncols = 3\n",
    "w_h_scale = 2\n",
    "figsize = (ncols * size * w_h_scale, size)\n",
    "fig, axs = plt.subplots(nrows=n//ncols, ncols=ncols, figsize=figsize)\n",
    "axs = axs.flatten()\n",
    "\n",
    "axs[0].plot(history['val_accuracy'], label='val_accuracy')\n",
    "axs[0].plot(history['accuracy'], label='accuracy')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(history['val_loss'], label='val_loss')\n",
    "axs[1].plot(history['loss'], label='loss')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(history['lr'])\n",
    "axs[2].set_xlabel('epoch')\n",
    "\n",
    "fig.savefig(model_path / 'history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = model\n",
    "\n",
    "# Perform prediction\n",
    "model_image_size = (299, 299)\n",
    "n  = 8\n",
    "fnames = [str(p) for p in val_data_dir.glob(\"*.tfrecord\")]\n",
    "dataset = tf.data.TFRecordDataset(fnames)\n",
    "dataset = dataset.map(map_func=captchanet.decode_data(tokenizer, max_len_word=10, image_size=model_image_size, input_as_dict=True))\n",
    "dataset = dataset.shuffle(1024)\n",
    "dataset = dataset.batch(n)\n",
    "data = [d for d in dataset.take(1)][0]\n",
    "\n",
    "images = data['image']\n",
    "labels = data['label']\n",
    "\n",
    "labels = loaded_model(images)\n",
    "\n",
    "# Decode\n",
    "labels = labels.numpy().argmax(axis=2)\n",
    "predicted_words = [tokenizer.sequences_to_texts([label])[0] for label in labels]\n",
    "predicted_words = [word.replace(' ', '') for word in predicted_words]\n",
    "\n",
    "# Plot\n",
    "original_images = data['original_image'].numpy()\n",
    "words = data['word'].numpy()\n",
    "words = [w.decode('utf-8').replace('0', '') for w in words]\n",
    "\n",
    "size = 2\n",
    "ncols = 2\n",
    "nrows = n // ncols\n",
    "ratio = original_images.shape[2] / original_images.shape[1]\n",
    "figsize = (ncols * size * ratio, size * nrows)\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for image, word, predicted_word, ax in zip(original_images, words, predicted_words, axs):\n",
    "  ax.imshow(image)\n",
    "  \n",
    "  mark = 'OK' if predicted_word == word else 'WRONG'\n",
    "  text = f'True: {word} ({len(word)})'\n",
    "  text += f' - Predicted: {predicted_word} ({len(predicted_word)})'\n",
    "  text += f\" - {mark}\"\n",
    "  ax.set_title(text, fontsize=14)\n",
    "  \n",
    "fig.savefig(model_path / 'example_prediction.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:captchanet]",
   "language": "python",
   "name": "conda-env-captchanet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
