{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import io\n",
    "import urllib.parse as urlparse\n",
    "import string\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tqdm.auto import trange\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "from scrapy.http import TextResponse\n",
    "from PIL import Image\n",
    "\n",
    "model_dir = Path(\"/home/hadim/.data/Neural_Network/captchanet/model/\")\n",
    "\n",
    "model_name = 'v5'\n",
    "model_path = model_dir / model_name\n",
    "tokenizer_path = model_path / 'tokenizer.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Download random images and run the model on it. Here we don't have access to the true captcha label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions on randomly downloaded captcha images.\n",
    "\n",
    "# Load tokenizer and model\n",
    "with open(model_path / 'tokenizer.json') as f:\n",
    "  #tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n",
    "  from keras_preprocessing import text\n",
    "  tokenizer = text.tokenizer_from_json(f.read())\n",
    "model = tf.keras.models.load_model(str(model_path / 'model'))\n",
    "\n",
    "# Download images\n",
    "batch_size = 8\n",
    "captcha_url = \"https://www.referendum.interieur.gouv.fr/bundles/ripconsultation/securimage/securimage_show.php\"\n",
    "images = [np.array(Image.open(io.BytesIO(requests.get(captcha_url).content))) for _ in trange(batch_size)]\n",
    "images = np.array(images)\n",
    "\n",
    "# Preprocess the image\n",
    "# The images need to be normalized t [0, 1] and resize to (224, 224).\n",
    "image_size = (224, 224)\n",
    "batch = tf.cast(images, 'float32')\n",
    "batch = tf.image.per_image_standardization(batch)\n",
    "batch = tf.image.resize(batch, image_size)\n",
    "\n",
    "# Run inference\n",
    "labels = model(batch)\n",
    "\n",
    "# Postprocess results (decode labels)\n",
    "labels = tf.argmax(labels, axis=2)\n",
    "words = tokenizer.sequences_to_texts(labels.numpy())\n",
    "words = [word.replace(' ', '') for word in words]\n",
    "\n",
    "fig_size = 2\n",
    "ncols = 2\n",
    "nrows = batch_size // ncols\n",
    "ratio = images.shape[2] / images.shape[1]\n",
    "figsize = (ncols * fig_size * ratio, fig_size * nrows)\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for image, ax, word in zip(images, axs, words):\n",
    "  ax.imshow(image)\n",
    "  text = f'Predicted: {word} ({len(word)})'\n",
    "  ax.set_title(text, fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Here we try to solve the captcha by postinf the captcha answer to the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "with open(model_path / 'tokenizer.json') as f:\n",
    "  #tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(f.read())\n",
    "  from keras_preprocessing import text\n",
    "  tokenizer = text.tokenizer_from_json(f.read())\n",
    "model = tf.keras.models.load_model(str(model_path / 'model'))\n",
    "\n",
    "def captcha_solver(image):\n",
    "    \n",
    "  image = np.array(image)\n",
    "  images = np.array([image])\n",
    "  \n",
    "  # The images need to be normalized t [0, 1] and resize to (224, 224).\n",
    "  image_size = (224, 224)\n",
    "  batch = tf.cast(images, 'float32')\n",
    "  batch = tf.image.per_image_standardization(batch)\n",
    "  batch = tf.image.resize(batch, image_size)\n",
    "\n",
    "  # Run inference\n",
    "  labels = model(batch)\n",
    "\n",
    "  # Postprocess results (decode labels)\n",
    "  labels = tf.argmax(labels, axis=2)\n",
    "  words = tokenizer.sequences_to_texts(labels.numpy())\n",
    "  words = [word.replace(' ', '') for word in words]\n",
    "  \n",
    "  return words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_url = \"https://www.referendum.interieur.gouv.fr/consultation_publique/8/\"\n",
    "\n",
    "n = 500\n",
    "success = 0\n",
    "rate = 0\n",
    "failed_images = []\n",
    "failed_words = []\n",
    "\n",
    "for i in trange(n):\n",
    "\n",
    "  letter1 = random.choice(string.ascii_uppercase)\n",
    "  letter2 = random.choice(string.ascii_uppercase)\n",
    "  letter3 = random.choice(string.ascii_uppercase)\n",
    "  url = urlparse.urljoin(root_url, f'{letter1}/{letter2}{letter3}')\n",
    "\n",
    "  cookies = {}\n",
    "  cookies['incap_ses_700_2043128'] = 'z00hQBP2mS515CRSzvK2CarcKV0AAAAA54RCSzqGm0bptUV4OJjvlA=='\n",
    "  session = requests.Session()\n",
    "  requests.utils.add_dict_to_cookiejar(session.cookies, cookies)\n",
    "\n",
    "  req = session.get(url)\n",
    "  response = TextResponse(str(req.url), body=req.text, encoding='utf-8')\n",
    "  \n",
    "  iframe_src = response.css('iframe').xpath('@src').get()\n",
    "  if iframe_src and 'Incapsula' in iframe_src:\n",
    "    raise Exception(\"Incapsula issue.\")\n",
    "\n",
    "  captcha = response.css('img#captcha').xpath('@src').get()\n",
    "\n",
    "  if not captcha:\n",
    "    raise Exception(\"Captcha not here.\")\n",
    "\n",
    "  # Get the token\n",
    "  token = response.css('#form__token').xpath('@value').get()\n",
    "\n",
    "  # Get the image captcha URL\n",
    "  captcha_uri = response.css('img#captcha').xpath('@src').get()\n",
    "  captcha_url = urlparse.urljoin(root_url, captcha_uri)\n",
    "\n",
    "  # Download the image\n",
    "  req = session.get(captcha_url)\n",
    "  captcha_image = Image.open(io.BytesIO(req.content))\n",
    "\n",
    "  # Solve the captcha\n",
    "  captcha_solution = captcha_solver(captcha_image)\n",
    "\n",
    "  # Send captcha solution.\n",
    "  form_data = {}\n",
    "  form_data['form[captcha]'] = captcha_solution\n",
    "  form_data['form[_token]'] = token\n",
    "\n",
    "  # Get the actual page.\n",
    "  req = session.post(url, data=form_data)\n",
    "  response = TextResponse(str(req.url), body=req.text, encoding='utf-8')\n",
    "  \n",
    "  iframe_src = response.css('iframe').xpath('@src').get()\n",
    "  if iframe_src and 'Incapsula' in iframe_src:\n",
    "    raise Exception(\"Incapsula issue.\")\n",
    "\n",
    "  captcha = response.css('img#captcha').xpath('@src').get()\n",
    "\n",
    "  if not captcha:\n",
    "    success += 1\n",
    "  else:\n",
    "    failed_images.append(np.asarray(captcha_image))\n",
    "    failed_words.append(np.asarray(captcha_solution))\n",
    "    \n",
    "  rate = (success / (i+1)) * 100\n",
    "  tqdm.write(f\"{rate:.1f} ({success}/{i+1})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:captchanet]",
   "language": "python",
   "name": "conda-env-captchanet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
